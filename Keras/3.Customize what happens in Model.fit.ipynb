{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 상황에 맞게 맞춤 설정\n",
    "---\n",
    "\n",
    "fit()은 굉장히 편리합니다만, 자신만의 학습 루프를 작성 해야할 때가 존재합니다.\n",
    "\n",
    "GradientTape는 자동 미분을 계산해주는 방식으로, tensorflow2에선, Tape에 Gradient를 저장하는 방식으로 Backpropagation을 계산해줍니다.\n",
    "\n",
    "이 챕터에서는 fit()의 기능을 계속 활용하면서, 맞춤 설정하는 방법을 배웁니다.\n",
    "\n",
    "\n",
    "Keras의 핵슴 원칙은 복잡성을 점진적으로 공개하는것입니다.\n",
    "\n",
    "높은 수준의 편의성을 유지하면서 작은 세부사항을 더 잘 제어할 수 있어야합니다.\n",
    "(고수준 api로 안되는 부분부분만 저수준으로 접근 가능하도록 만들었다 정도?)\n",
    "\n",
    "fit()을 사용자 정의 해야하는 경우 Model 클래스의 학습 단계 함수를 재정의 해야합니다.\n",
    "\n",
    "그러면 평소처럼 fit()을 호출할 수 있으며 자체 학습 알고리즘을 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model subclassing\n",
    "---\n",
    "\n",
    "- keras.Model을 상속\n",
    "- train_step(self, data) 메서드를 재 정의\n",
    "- 딕셔너리 매핑 메트릭 이름을 현재 값으로 반환\n",
    "\n",
    "train_step은 fit()와 유사한 업데이트를 구현합니다.\n",
    "\n",
    "self.compiled_loss를 통해 loss를 계산합니다. 이는 compile()로 전달 된 loss함수를 래핑합니다.\n",
    "\n",
    "self.compiled_metrics.update_state(y, y_pred)를 호출하여 compile()에서 전달된 메트릭의 상태를 업데이트 하고\n",
    "\n",
    "self.metrics 결과를 쿼리하여 현재 값을 검색합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(keras.Model):\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True) #Forward pass\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "        \n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # Update Wegiths\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        #Update Metrics\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        \n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2298 - mae: 0.3862\n",
      "Epoch 2/3\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2149 - mae: 0.3742\n",
      "Epoch 3/3\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2044 - mae: 0.3645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9895dfb240>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Construct and compile an instance of CustomModel\n",
    "inputs = keras.Input(shape=(32,))\n",
    "outputs = keras.layers.Dense(1)(inputs)\n",
    "model = CustomModel(inputs, outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "# Just use `fit` as usual\n",
    "x = np.random.random((1000, 32))\n",
    "y = np.random.random((1000, 1))\n",
    "model.fit(x, y, epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

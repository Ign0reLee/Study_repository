{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 상황에 맞게 맞춤 설정\n",
    "---\n",
    "\n",
    "fit()은 굉장히 편리합니다만, 자신만의 학습 루프를 작성 해야할 때가 존재합니다.\n",
    "\n",
    "GradientTape는 자동 미분을 계산해주는 방식으로, tensorflow2에선, Tape에 Gradient를 저장하는 방식으로 Backpropagation을 계산해줍니다.\n",
    "\n",
    "이 챕터에서는 fit()의 기능을 계속 활용하면서, 맞춤 설정하는 방법을 배웁니다.\n",
    "\n",
    "\n",
    "Keras의 핵슴 원칙은 복잡성을 점진적으로 공개하는것입니다.\n",
    "\n",
    "높은 수준의 편의성을 유지하면서 작은 세부사항을 더 잘 제어할 수 있어야합니다.\n",
    "(고수준 api로 안되는 부분부분만 저수준으로 접근 가능하도록 만들었다 정도?)\n",
    "\n",
    "fit()을 사용자 정의 해야하는 경우 Model 클래스의 학습 단계 함수를 재정의 해야합니다.\n",
    "\n",
    "그러면 평소처럼 fit()을 호출할 수 있으며 자체 학습 알고리즘을 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model subclassing\n",
    "---\n",
    "\n",
    "- keras.Model을 상속\n",
    "- train_step(self, data) 메서드를 재 정의\n",
    "- 딕셔너리 매핑 메트릭 이름을 현재 값으로 반환\n",
    "\n",
    "train_step은 fit()와 유사한 업데이트를 구현합니다.\n",
    "\n",
    "self.compiled_loss를 통해 loss를 계산합니다. 이는 compile()로 전달 된 loss함수를 래핑합니다.\n",
    "\n",
    "self.compiled_metrics.update_state(y, y_pred)를 호출하여 compile()에서 전달된 메트릭의 상태를 업데이트 하고\n",
    "\n",
    "self.metrics 결과를 쿼리하여 현재 값을 검색합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(keras.Model):\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True) #Forward pass\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "        \n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # Update Wegiths\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        #Update Metrics\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        \n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7390 - mae: 0.7548\n",
      "Epoch 2/3\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2713 - mae: 0.4203\n",
      "Epoch 3/3\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1761 - mae: 0.3396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7c9f59a240>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Construct and compile an instance of CustomModel\n",
    "inputs = keras.Input(shape=(32,))\n",
    "outputs = keras.layers.Dense(1)(inputs)\n",
    "model = CustomModel(inputs, outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "# Just use `fit` as usual\n",
    "x = np.random.random((1000, 32))\n",
    "y = np.random.random((1000, 1))\n",
    "model.fit(x, y, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 낮은 수준으로 이동\n",
    "---\n",
    "\n",
    "Compile()로 인수를 전달하지 않아도 train_step에서 수동으로 모든 작업을 수행할 수 있습니다.\n",
    "\n",
    "메트릭 역시 마찬가지입니다.\n",
    "\n",
    "-  MAE Metric 인스턴스를 만들어봅니다.\n",
    "-  trian__step()을 호출하여 update_state()를 호출, 메트릭 상태를 업데이트 후에, 쿼리 result()를 반환 진행률 표시줄에 의해 표시할 수 있는 모든 콜백에 전달합니다.\n",
    "-  reset_states() 사이의 메트릭에 대해선 reset_states()를 호출해야합니다. 하지않으면 일반적으로 에포크당 평균의 metric을 반환합니다.\n",
    "-  재설정하는 metrics의 속성에 나열하면됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "mae_metric = keras.metrics.MeanAbsoluteError(name = \"mae\")\n",
    "\n",
    "class CustomModel(keras.Model):\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True) #Forward pass\n",
    "            loss = keras.losses.mean_squared_error(y, y_pred)\n",
    "        \n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # Update Wegiths\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        #Update Metrics\n",
    "        loss_tracker.update_state(loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        \n",
    "        return {\"loss\":  loss_tracker.result() ,\"mae\" : mae_metric.result()}\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our 'Metric' objects here so that 'reset_states()' can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of 'evaluate()'\n",
    "        # If you don't implement this property, you have to call\n",
    "        # 'reset_states()' yourself at the time of your choosing.\n",
    "        return [loss_tracker, mae_metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5978 - mae: 0.0000e+00\n",
      "Epoch 2/3\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2910 - mae: 0.0000e+00\n",
      "Epoch 3/3\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2543 - mae: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7c7c056c88>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct and compile an instance of CustomModel\n",
    "inputs = keras.Input(shape=(32,))\n",
    "outputs = keras.layers.Dense(1)(inputs)\n",
    "model = CustomModel(inputs, outputs)\n",
    "\n",
    "# We don't passes a loss or metrics here.\n",
    "model.compile(optimizer=\"adam\")\n",
    "\n",
    "# Just use `fit` as usual\n",
    "x = np.random.random((1000, 32))\n",
    "y = np.random.random((1000, 1))\n",
    "model.fit(x, y, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampple Weight & Class Weight\n",
    "---\n",
    "\n",
    "fit()를 이용하여 학습 씨 sample_weight와 class_weight도 설정가능합니다.\n",
    "\n",
    "- Unpack sample_weight from the data argument\n",
    "- Pass compiled_loss & compiled_metrics(물론, compile()을 사용하지 않고 직접 작성해도 괜찮습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        if len(data) ==3:\n",
    "            x, y, sample_weight = data\n",
    "        else:\n",
    "            x, y = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True) #Forward pass\n",
    "            loss = self.compiled_loss(\n",
    "                y,\n",
    "                y_pred,\n",
    "                sample_weight = sample_weight,\n",
    "                regularization_losses=self.losses)\n",
    "        \n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # Update Wegiths\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        #Update Metrics\n",
    "        #Metrics are configured in compile()\n",
    "        self.compiled_metrics.update_state(y, y_pred, sample_weight= sample_weight)\n",
    "        \n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1299 - mae: 0.4134\n",
      "Epoch 2/3\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1125 - mae: 0.3784\n",
      "Epoch 3/3\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1079 - mae: 0.3739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7c1c598828>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(32,))\n",
    "outputs = keras.layers.Dense(1)(inputs)\n",
    "model = CustomModel(inputs, outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "# Just use `fit` as usual\n",
    "x = np.random.random((1000, 32))\n",
    "y = np.random.random((1000, 1))\n",
    "sw = np.random.random((1000, 1))\n",
    "model.fit(x, y, sample_weight=sw, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자신만의 Evaluation Step 제공하기\n",
    "---\n",
    "\n",
    "model.evaluate()에서 일어나는 일들을 조절하기 위해선, test_step 명령어를 사용하여 진행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(keras.Model):\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=False) #Forward pass\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "        \n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # Update Wegiths\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        #Update Metrics\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        \n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2047 - mae: 0.3647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2047402262687683, 0.36473095417022705]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct and compile an instance of CustomModel\n",
    "inputs = keras.Input(shape=(32,))\n",
    "outputs = keras.layers.Dense(1)(inputs)\n",
    "model = CustomModel(inputs, outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "# Just use `evaluate` as usual\n",
    "x = np.random.random((1000, 32))\n",
    "y = np.random.random((1000, 1))\n",
    "model.evaluate(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
